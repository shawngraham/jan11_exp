#!/usr/bin/env python3
"""
OCR Processing Script - DocTR (Vision Transformer) Edition
Reads preprocessed column images and recognizes text using Deep Learning.
"""

import json
import sys
import gc
import cv2
import numpy as np
from pathlib import Path

# Step 1: Dependencies - requires `pip install python-doctr[torch]`
try:
    from doctr.models import ocr_predictor
    from doctr.io import DocumentFile
except ImportError:
    print("Error: DocTR not installed. Run: pip install python-doctr[torch]")
    sys.exit(1)

# Custom Encoder to handle NumPy types from the OCR model
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (np.int64, np.int32, np.float32, np.float64)):
            return float(obj)
        return json.JSONEncoder.default(self, obj)

# Initialize the DocTR model (Vision Transformer)
# det_arch: 'db_resnet50' (Finds text blocks)
# reco_arch: 'crnn_vgg16_bn' (Reads the actual characters)
print("Initializing DocTR Vision Transformer...")
model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)

def resolve_image_path(stored_path, project_root):
    """
    Ensures we find the image even if the path in JSON is relative.
    """
    p = Path(stored_path)
    if p.exists():
        return p
    # Try relative to project root
    alt_p = project_root / stored_path
    if alt_p.exists():
        return alt_p
    # Try relative to 'data/preprocessed'
    alt_p2 = project_root / "data" / "preprocessed" / p.name
    if alt_p2.exists():
        return alt_p2
    return None

def process_column(col_meta, project_root):
    """
    Loads a single preprocessed column crop and runs Vision OCR.
    """
    img_path = resolve_image_path(col_meta['path'], project_root)
    if not img_path:
        print(f"\n      [!] Warning: Could not find image at {col_meta['path']}")
        return []

    # Load image to get dimensions for coordinate conversion
    img_cv = cv2.imread(str(img_path))
    if img_cv is None: return []
    h_px, w_px = img_cv.shape[:2]

    # Convert image for DocTR and run prediction
    doc = DocumentFile.from_images(str(img_path))
    result = model(doc)
    
    # Export results (this structure contains lines and words)
    # Coordinate system is normalized [0, 1]
    output = result.export()
    
    text_blocks = []
    x_offset = col_meta['x_offset']

    # Navigate the DocTR output: Page -> Block -> Line
    for block in output['pages'][0]['blocks']:
        for line in block['lines']:
            # Join words in line
            line_text = " ".join([word['value'] for word in line['words']])
            
            # DocTR Geometry: ((ymin, xmin), (ymax, xmax))
            (ymin, xmin), (ymax, xmax) = line['geometry']
            
            # Convert normalized back to Broadhseet Pixels
            pixel_x = (xmin * w_px) + x_offset
            pixel_y = (ymin * h_px)
            pixel_w = (xmax - xmin) * w_px
            pixel_h = (ymax - ymin) * h_px

            text_blocks.append({
                "text": line_text,
                "bbox": {
                    "x": float(pixel_x),
                    "y": float(pixel_y),
                    "width": float(pixel_w),
                    "height": float(pixel_h)
                },
                "column": int(col_meta['column_index'])
            })
            
    return text_blocks

def main():
    # 1. Setup Project Paths
    script_dir = Path(__file__).parent
    project_root = script_dir if (script_dir / "data").exists() else script_dir.parent
    
    preprocessed_dir = project_root / "data" / "preprocessed"
    output_dir = project_root / "data" / "raw"
    output_dir.mkdir(parents=True, exist_ok=True)

    # 2. Load the metadata generated by Step 1
    metadata_path = preprocessed_dir / "all_metadata.json"
    if not metadata_path.exists():
        print(f"Error: Run preprocess.py first. Missing: {metadata_path}")
        return

    with open(metadata_path, 'r') as f:
        all_metadata = json.load(f)

    all_results = []
    print(f"Processing {len(all_metadata)} PDFs using Vision Transformer OCR...")

    for pdf_meta in all_metadata:
        pdf_name = pdf_meta['source_pdf']
        print(f"\nOCRing: {pdf_name}")
        
        pdf_entry = {"filename": pdf_name, "pages": []}

        for page_meta in pdf_meta['pages']:
            print(f"  Page {page_meta['page_num']}...", end="", flush=True)
            page_text = []
            
            # Iterate through the preprocessed COLUMN images
            for col_meta in page_meta['columns']:
                blocks = process_column(col_meta, project_root)
                page_text.extend(blocks)
                print(".", end="", flush=True) # Progress indicator
                gc.collect() # Essential: Vision models eat RAM
            
            pdf_entry["pages"].append({
                "page_number": int(page_meta['page_num']),
                "text_blocks": page_text,
                "total_blocks": len(page_text)
            })
            print(f" Done ({len(page_text)} lines)")

        all_results.append(pdf_entry)

    # 3. Save the consolidated results for the next pipeline steps
    output_file = output_dir / "ocr_output.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({"pdfs": all_results}, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)

    print(f"\nâœ“ Vision OCR complete. Results saved to {output_file}")

if __name__ == "__main__":
    main()